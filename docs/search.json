{
  "articles": [
    {
      "path": "about.html",
      "title": "About CORE",
      "description": "Computerized Oral Reading Evaluation - [CORE](https://ies.ed.gov/funding/grantsearch/details.asp?ID=1492) - is a project funded by the Institute of Education Sciences [(IES)](https://ies.ed.gov/) to develop and validate a new computerized assessment of oral reading fluency that is administered and scored online.",
      "author": [],
      "contents": "\r\nPurpose\r\nCORE uses an automated speech recognition engine and an an advanced psychometric model to overcome some of the inadequacies of traditional oral reading fluency assessments. CORE’s online delivery and automated speech recognition allows for whole classrooms to be tested at one time, which reduces administration time and cost, and minimizes administration errors by standardizing the delivery, setting, and scoring. CORE’s shorter passages and advanced psychometric model increases score reliability and provides teachers with more accurate oral reading fluency scores. CORE also equates and links passages, placing scores that are on the same scale across across Grades 2 through 4 (i.e., vertically linked) so that student oral reading fluency scores and growth can be evaluated across grades. CORE is one part of a larger effort to help improve systems for data-based decisions.\r\nProject Scope\r\nThe CORE project consists of four phases.\r\nContent & Convergent Evidence\r\nValidate the CORE reading passages and the ASR scoring.\r\n\r\nPsychometric Modeling\r\nDevelop a psychometric model that estimates oral reading fluency by incorporating student response time and response accuracy simultaneously.\r\n\r\nCalibration, Equating, Linking\r\nEquate and link the passage parameters within and across Grades 2 - 4.\r\n\r\nGrowth & Predictive Validity\r\nCompare the properties of CORE versus traditional oral reading fluency scores using (a) a longitudinal study design, and (b) predictive validity and classification accuracy of state test reading data and comprehension scores.\r\n\r\nInvestigators\r\nPI\r\nJoseph F. T. Nese is a Research Associate Professor at the University of Oregon with Behavioral Research and Teaching. He received his Ph.D. in school psychology from the University of Maryland in 2009, and his B.A from the University of California at Santa Barbara in 2002. His research involves educational assessment and applied measurement, focusing on developing and improving systems that support data-based decision making, and using advanced statistical methods to measure and monitor student growth.\r\nCo-PI\r\nAkihito Kamata is a Professor at Southern Methodist University (Department of Education Policy & Leadership, and Department of Psychology), and the Executive Director at the Center on Research and Evaluation. His primary research interest is psychometrics and educational and psychological measurement, focusing on implementation of item-level test data analysis methodology through various modeling framework, including item response theory, multilevel modeling, and structural equation modeling.\r\nFunding Source\r\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305A140203 to the University of Oregon. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-06-24T14:50:25-07:00"
    },
    {
      "path": "index.html",
      "title": "CORE Study Blog",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-06-24T20:42:30-07:00"
    },
    {
      "path": "LICENSE.html",
      "author": [],
      "contents": "\r\nThe MIT License (MIT)\r\nCopyright (c) 2019 Joe Nese\r\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\r\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\r\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\r\n\r\n\r\n",
      "last_modified": "2021-06-24T14:50:29-07:00"
    },
    {
      "path": "publications.html",
      "title": "CORE Publications",
      "description": "Computerized Oral Reading Evaluation - [CORE](https://ies.ed.gov/funding/grantsearch/details.asp?ID=1492) - is a project funded by the Institute of Education Sciences [(IES)](https://ies.ed.gov/) to develop and validate a new computerized assessment of oral reading fluency that is administered and scored online.",
      "author": [],
      "contents": "\r\nEvidence for Automated Scoring and Shorter Passages of CBM-R in Early Elementary School\r\nNese, J. F. T., & Kamata, A. (2021). Evidence for automated scoring and shorter passages of CBM-R in early elementary school. School Psychology, 36 (1), 47–59. doi: 10.1037/spq0000415\r\nCurriculum-based measurement of oral reading fluency (CBM-R) is widely used across the United States as a strong indicator of comprehension and overall reading achievement, but has several limitations including errors in administration and large standard errors of measurement. The purpose of this study is to compare scoring methods and passage lengths of CBM-R in an effort to evaluate potential improvements upon traditional CBM-R limitations. For a sample of 902 students in Grades 2 through 4, who collectively read 13,766 passages, we used mixed-effect models to estimate differences in CBM-R scores and examine the effects of (a) scoring method (comparing a human scoring criterion vs. traditional human or automatic speech recognition [ASR] scoring), and (b) passage length (25, 50, or 85 words, and traditional CBM-R length). We also examined differences in word score (correct/incorrect) agreement rates between human-to-human scoring and human-to-ASR scoring. Our results indicated that ASR can be applied in schools to score CBM-R, and that scores for shorter passages are comparable to traditional passages.\r\nEstimating Model-Based Oral Reading Fluency: A Bayesian Approach\r\nKara, Y., Kamata, A., Potgieter, C., & Nese, J. F. (2020). Estimating model-based oral reading fluency: A Bayesian approach. Educational and Psychological Measurement, 80 (5), 847-869. doi: 10.1177/0013164419900208\r\nOral reading fluency (ORF), used by teachers and school districts across the countryto screen and progress monitor at-risk readers, has been documented as a good indi-cator of reading comprehension and overall reading competence. In traditional ORFadministration, students are given one minute to read a grade-level passage, afterwhich the assessor calculates the words correct per minute (WCPM) fluency scoreby subtracting the number of incorrectly read words from the total number of wordsread aloud. As part of a larger effort to develop an improved ORF assessment sys-tem, this study expands on and demonstrates the performance of a new model-basedestimate of WCPM based on a recently developed latent-variable psychometricmodel of speed and accuracy for ORF data. The proposed method was applied to adata set collected from 58 fourth-grade students who read four passages (a total of260 words). The proposed model-based WCPM scores were also evaluated througha simulation study with respect to sample size and number of passages read.\r\nAddressing the Large Standard Error of Traditional CBM-R: Estimating the Conditional Standard Error of a Model-Based Estimate of CBM-R\r\nNese, J. F., & Kamata, A. (2020). Addressing the large standard error of traditional CBM-R: Estimating the conditional standard error of a model-based estimate of CBM-R. Assessment for Effective Intervention. doi: 10.1177/1534508420937801\r\nCurriculum-based measurement of oral reading fluency (CBM-R) is widely used across the country as a quick measure of reading proficiency that also serves as a good predictor of comprehension and overall reading achievement, but has several practical and technical inadequacies, including a large standard error of measurement (SEM). Reducing the SEM of CBM-R scores has positive implications for educators using these measures to screen or monitor student growth. The purpose of this study was to compare the SEM of traditional CBM-R words correct per minute (WCPM) fluency scores and the conditional SEM (CSEM) of model-based WCPM estimates, particularly for students with or at risk of poor reading outcomes. We found (a) the average CSEM for the model-based WCPM estimates was substantially smaller than the reported SEMs of traditional CBM-R systems, especially for scores at/below the 25th percentile, and (b) a large proportion (84%) of sample scores, and an even larger proportion of scores at/below the 25th percentile (about 99%) had a smaller CSEM than the reported SEMs of traditional CBM-R systems.\r\nComparing the Growth and Predictive Performance of a Traditional Oral Reading Fluency Measure to an Experimental Novel Measure\r\nUnder Review\r\nCurriculum-based measurement of oral reading fluency (CBM-R) is used as an indicator of reading proficiency, and to measure at risk students’ response to reading interventions to help ensure effective instruction. The purpose of this study was to compare model-based WCPM scores (CORE) to Traditional CBM-R WCPM scores to determine which provides more reliable growth estimates and demonstrates better predictive performance of reading comprehension and state reading test scores. Results indicated that in general, CORE had better (a) within-growth properties (smaller SDs of slope estimates and higher reliability), and (b) predictive performance (lower RMSE, and higher \\(R^2\\), sensitivity, specificity, and AUC values). These results suggest increased measurement precision for the model-based CORE scores compared to Traditional CBM-R, providing preliminary evidence that CORE can be used for consequential assessment.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-06-25T13:01:37-07:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
